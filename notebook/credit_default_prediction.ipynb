{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e01850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "EXPORT_PLOTS = False  # Set True only when you want to save figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e7ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/loan_data.csv')\n",
    "df = df.set_index('customer_id')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target balance\n",
    "print(df['default'].value_counts(normalize=True))\n",
    "sns.countplot(x='default', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "pair_plot = sns.pairplot(df, hue='default')\n",
    "plt.show()\n",
    "\n",
    "if EXPORT_PLOTS:\n",
    "    pair_plot.savefig(\"../outputs/credit_data_pairplot.png\", dpi=300,\n",
    "                      bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "figsize = (12, 8)\n",
    "ax = sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm_r',\n",
    "                 vmin=-1, vmax=1)\n",
    "fig = ax.get_figure()\n",
    "plt.show()\n",
    "\n",
    "if EXPORT_PLOTS:\n",
    "    fig.savefig(\"../outputs/credit_data_correl.png\", dpi=300,\n",
    "                bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143fd4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "X = df.drop(columns=['default'])\n",
    "y = df['default']\n",
    "\n",
    "# Train-test split\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_df)\n",
    "X_test = scaler.transform(X_test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression Pipeline\n",
    "poly_log_reg = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('log_reg', LogisticRegression(max_iter=500))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bca13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models dictionnary\n",
    "models = {\n",
    "    \"Dummy Classifier\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"Polynomial Regression (deg=2)\": poly_log_reg,\n",
    "    \"Decision Tree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight=\"balanced\", random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d830a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models and evaluate\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "\n",
    "    # Fit\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict + proba\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    results[model_name] = {\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"MSE\": mse,\n",
    "        \"Confusion Matrix\": cm\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"ROC AUC: {metrics['ROC AUC']:.4f}\")\n",
    "    print(f\"MSE: {metrics['MSE']:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(metrics['Confusion Matrix'])\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f50db09",
   "metadata": {},
   "source": [
    "### Model Comparison and Interpretation\n",
    "\n",
    "Below are some notes to help interpret the results you observed:\n",
    "\n",
    "#### 1. Dummy Classifier\n",
    "- Predicts only the majority class.\n",
    "- Explains the ROC AUC of 0.50.\n",
    "- Cannot detect any pattern in the data.\n",
    "\n",
    "#### 2. Logistic Regression\n",
    "- Performs extremely well here (AUC = 1.00).\n",
    "- Works best when the true decision boundary is close to linear.\n",
    "- Can be influenced by outliers because coefficients shift to fit extreme points.\n",
    "\n",
    "#### 3. Polynomial Regression (degree=2)\n",
    "- Adds interactions and non-linearities, which increases flexibility.\n",
    "- Can fit noise and outliers too strongly due to squared terms.\n",
    "- Here it performs almost perfectly, meaning the dataset is simple or clean.\n",
    "\n",
    "#### 4. Decision Tree\n",
    "- Very flexible and learns sharp rules.\n",
    "- Highly sensitive to outliers: a single extreme point can create an unnecessary split.\n",
    "- Slightly higher MSE and a few more misclassifications confirm mild overfitting.\n",
    "\n",
    "#### 5. KNN\n",
    "- Distance-based model: outliers distort neighborhood structure.\n",
    "- Works well on clean, dense data, but high dimensions or unscaled features hurt performance.\n",
    "- The confusion matrix suggests a few more false negatives caused by mis-localized neighbors.\n",
    "\n",
    "#### 6. Random Forest\n",
    "- More stable than a single tree due to averaging.\n",
    "- Still somewhat sensitive if outliers repeatedly affect splits across trees.\n",
    "- Strong performance suggests features are clean and informative.\n",
    "\n",
    "#### 7. Gradient Boosting\n",
    "- Sequentially focuses on correcting previous errors.\n",
    "- Can overfit outliers if not tuned, because it keeps pushing on difficult points.\n",
    "- Very strong AUC indicates the dataset is easy to separate.\n",
    "\n",
    "### Why outliers matter in these models\n",
    "\n",
    "- **Linear models:** boundary shifts to accommodate extreme points.\n",
    "- **Polynomial:** extremes are amplified through squared terms.\n",
    "- **Trees:** outliers trigger deep splits, creating overfitting.\n",
    "- **KNN:** distances become misleading.\n",
    "- **Ensembles:** more robust, but still influenced if the same outliers appear often.\n",
    "\n",
    "Overall, the dataset seems extremely separable, which explains why most models reach very high ROC AUC values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d2e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results dict â†’ DataFrame\n",
    "html_rows = []\n",
    "for model_name, metrics in results.items():\n",
    "    cm = metrics[\"Confusion Matrix\"]\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    html_rows.append({\n",
    "        \"Model\": model_name,\n",
    "        \"ROC AUC\": round(metrics[\"ROC AUC\"], 4),\n",
    "        \"MSE\": round(metrics[\"MSE\"], 4),\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn,\n",
    "        \"TP\": tp\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(html_rows)\n",
    "\n",
    "# Save as HTML file (GitHub-friendly)\n",
    "html_path = \"../outputs/model_results.html\"\n",
    "df_results.to_html(html_path, index=False)\n",
    "\n",
    "print(f\"Saved HTML summary to {html_path}\")\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daad3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_loss(model, X_input, loan_amount, recovery_rate=0.1):\n",
    "    pd = model.predict_proba(X_input)[:, 1]  # Probability of Default\n",
    "    ead = loan_amount                        # Exposure at Default\n",
    "    lgd = 1 - recovery_rate                  # Loss Given Default\n",
    "    el = pd * lgd * ead                      # Expected Loss\n",
    "    return el[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "example_loan = X_test[0].reshape(1, -1)\n",
    "loan_amount = X_test_df.loc[X_test_df.index[0], 'loan_amt_outstanding']\n",
    "el = calculate_expected_loss(models['Logistic Regression'], example_loan,\n",
    "                             loan_amount)\n",
    "print(f\"Expected Loss for the example loan: {el:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-ml-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
